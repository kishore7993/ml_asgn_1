Explicit and Implicit Regularization in Neural Networks
A Comparative Study of L2 Weight Decay and Early Stopping
Overview

This repository accompanies the tutorial “Explicit and Implicit Regularization in Neural Networks: A Comparative Study of L2 Weight Decay and Early Stopping.”
The project investigates how different regularisation strategies influence training dynamics and generalisation behaviour in neural networks.

Two approaches are compared:

Explicit regularisation: L2 weight decay applied directly to the loss function

Implicit regularisation: Early stopping based on validation performance

Using a fixed neural network architecture and controlled experimental conditions, the repository demonstrates how both methods reduce overfitting while operating through distinct mechanisms.

Repository Structure
.
├── notebook/
│   └── regularization_comparison.ipynb
├── figures/
│   └── (generated automatically by the notebook)
├── README.md
├── LICENSE
└── requirements.txt

Description of Files

notebook/regularization_comparison.ipynb
Fully runnable Jupyter notebook containing:

Data generation

Model training

All experiments

All figures used in the tutorial

figures/
Directory for plots generated by the notebook (training curves, decision boundaries, etc.).

README.md
Instructions and project documentation (this file).

LICENSE
MIT license specifying usage permissions.

requirements.txt
Python dependencies required to run the notebook.

Dataset

A synthetic two-dimensional classification dataset is used, generated via sklearn.datasets.make_moons.

Number of samples: 1,500

Classes: 2

Noise added to create overfitting potential

This dataset was chosen because it is:

Nonlinearly separable

Easy to visualise

Well-suited for analysing regularisation effects

Experimental Setup

Model: Multilayer Perceptron (2 hidden layers, 64 units each)

Activation: ReLU

Optimiser: Stochastic Gradient Descent (SGD)

Loss: Binary cross-entropy

Regularisation strategies tested:

No regularisation (baseline)

L2 weight decay

Early stopping

All hyperparameters except the regularisation strategy are held constant to isolate its effect.

How to Run the Code
1. Clone the repository
git clone <your-github-repo-link>
cd <repo-name>

2. Install dependencies

It is recommended to use a virtual environment.

pip install -r requirements.txt

3. Run the notebook
jupyter notebook notebook/regularization_comparison.ipynb


Run all cells from top to bottom to reproduce:

Training and validation curves

Weight norm evolution

Decision boundary plots

Summary results table

Reproducibility

Fixed random seeds are used where applicable

All figures shown in the tutorial are generated directly from the notebook

No manual post-processing is required

Accessibility Considerations

High-contrast plots with clear labels and legends

Figures are interpretable without colour alone

Tables provide numerical summaries independent of visualisations

Clear sectioning supports screen reader navigation

License

This project is released under the MIT License.
You are free to use, modify, and distribute the code with appropriate attribution.
